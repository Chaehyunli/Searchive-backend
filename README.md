# Searchive Backend

Searchive í”„ë¡œì íŠ¸ì˜ í•µì‹¬ ë‘ë‡Œ ì—­í• ì„ í•˜ëŠ” Python/FastAPI ê¸°ë°˜ API ì„œë²„ì…ë‹ˆë‹¤. ì‚¬ìš©ì ì¸ì¦, ë¬¸ì„œ ê´€ë¦¬, ê²€ìƒ‰, ê·¸ë¦¬ê³  ì§€ëŠ¥í˜• RAG íŒŒì´í”„ë¼ì¸ì„ ì´ê´„í•©ë‹ˆë‹¤.

---

## âœ¨ ì•„í‚¤í…ì²˜: ë„ë©”ì¸ ì£¼ë„ ê³„ì¸µí˜• ì•„í‚¤í…ì²˜

ë³¸ í”„ë¡œì íŠ¸ëŠ” ìœ ì§€ë³´ìˆ˜ì„±ê³¼ í™•ì¥ì„±ì„ ê·¹ëŒ€í™”í•˜ê¸° ìœ„í•´ **ë„ë©”ì¸ ì£¼ë„ ì„¤ê³„(Domain-Driven Design)**ì˜ ê°œë…ì„ ë„ì…í•œ ê³„ì¸µí˜• ì•„í‚¤í…ì²˜ë¥¼ ë”°ë¦…ë‹ˆë‹¤. ëª¨ë“  ì†ŒìŠ¤ ì½”ë“œëŠ” `src/domains` í´ë” ì•„ë˜ì— ê° ê¸°ëŠ¥(ë„ë©”ì¸)ë³„ë¡œ ê·¸ë£¹í™”ë©ë‹ˆë‹¤.

### ê° ê³„ì¸µì˜ ì—­í• 

-   **`router.py` (Controller/API Layer)**: HTTP ìš”ì²­ì„ ë°›ì•„ ìœ íš¨ì„±ì„ ê²€ì‚¬í•˜ê³ , ì ì ˆí•œ ì„œë¹„ìŠ¤ë¡œ ìš”ì²­ì„ ì „ë‹¬í•˜ëŠ” API ì—”ë“œí¬ì¸íŠ¸ ê³„ì¸µì…ë‹ˆë‹¤.
-   **`schemas.py` (DTO Layer)**: Pydantic ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ API ìš”ì²­ ë° ì‘ë‹µì˜ ë°ì´í„° êµ¬ì¡°ë¥¼ ì •ì˜í•˜ê³  ìœ íš¨ì„±ì„ ê²€ì‚¬í•©ë‹ˆë‹¤.
-   **`services.py` (Service/Business Logic Layer)**: ì‹¤ì œ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì—¬ëŸ¬ ë¦¬í¬ì§€í† ë¦¬ë¥¼ ì¡°í•©í•˜ì—¬ ë³µì¡í•œ ì‘ì—…ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
-   **`repositories.py` (Data Access Layer)**: ë°ì´í„°ë² ì´ìŠ¤ì™€ì˜ ìƒí˜¸ì‘ìš©ì„ ë‹´ë‹¹í•˜ë©°, CRUD ì—°ì‚°ì„ ì¶”ìƒí™”í•©ë‹ˆë‹¤.
-   **`models.py` (Domain/Entity Layer)**: SQLAlchemy ORM ëª¨ë¸ë¡œ, ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸” êµ¬ì¡°ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.

---

## ğŸ“‚ í´ë” êµ¬ì¡°

```
Searchive-backend/
â”œâ”€â”€ .env                    # ì‹¤ì œ í™˜ê²½ ë³€ìˆ˜ íŒŒì¼ (Git ë¬´ì‹œ)
â”œâ”€â”€ .env_example            # í™˜ê²½ ë³€ìˆ˜ ì˜ˆì‹œ íŒŒì¼
â”œâ”€â”€ .gitignore              # Git ë¬´ì‹œ ëª©ë¡
â”œâ”€â”€ alembic/                # Alembic ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸ ì €ì¥ í´ë”
â”‚   â”œâ”€â”€ versions/           # ë§ˆì´ê·¸ë ˆì´ì…˜ ë²„ì „ íŒŒì¼ë“¤
â”‚   â”œâ”€â”€ env.py              # Alembic í™˜ê²½ ì„¤ì •
â”‚   â”œâ”€â”€ script.py.mako      # ë§ˆì´ê·¸ë ˆì´ì…˜ í…œí”Œë¦¿
â”‚   â””â”€â”€ README
â”œâ”€â”€ alembic.ini             # Alembic ì„¤ì • íŒŒì¼
â”œâ”€â”€ requirements.txt        # Python ì˜ì¡´ì„± ëª©ë¡
â”œâ”€â”€ pytest.ini              # Pytest ì„¤ì • íŒŒì¼
â”œâ”€â”€ README.md               # í”„ë¡œì íŠ¸ ì„¤ëª… íŒŒì¼
â”œâ”€â”€ tests/                  # í…ŒìŠ¤íŠ¸ ì½”ë“œ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py         # Pytest ì„¤ì • ë° í”½ìŠ¤ì²˜
â”‚   â”œâ”€â”€ README.md           # í…ŒìŠ¤íŠ¸ ê°€ì´ë“œ
â”‚   â”œâ”€â”€ unit/               # ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ domains/
â”‚   â”‚       â””â”€â”€ __init__.py
â”‚   â””â”€â”€ integration/        # í†µí•© í…ŒìŠ¤íŠ¸
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ domains/
â”‚       â”‚   â””â”€â”€ __init__.py
â”‚       â”œâ”€â”€ test_db_connection.py
â”‚       â””â”€â”€ test_redis_connection.py
â””â”€â”€ src/                    # ì†ŒìŠ¤ ì½”ë“œ ë£¨íŠ¸
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ main.py             # FastAPI ì•± ìƒì„± ë° ë¼ìš°í„° í¬í•¨
    â”œâ”€â”€ core/               # í”„ë¡œì íŠ¸ í•µì‹¬ ì„¤ì •
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ config.py       # .env íŒŒì¼ì„ ì½ì–´ì˜¤ëŠ” í™˜ê²½ ë³€ìˆ˜ ê´€ë¦¬
    â”‚   â”œâ”€â”€ exception.py    # ì˜ˆì™¸ ì²˜ë¦¬ í•¸ë“¤ëŸ¬
    â”‚   â”œâ”€â”€ redis.py        # Redis ì—°ê²° ë° ì„¸ì…˜ ê´€ë¦¬
    â”‚   â”œâ”€â”€ security.py     # ë³´ì•ˆ ê´€ë ¨ ìœ í‹¸ë¦¬í‹° (JWT ë“±)
    â”‚   â””â”€â”€ minio_client.py # MinIO í´ë¼ì´ì–¸íŠ¸ ìœ í‹¸ë¦¬í‹°
    â”œâ”€â”€ db/                 # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ë° ì„¸ì…˜ ê´€ë¦¬
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â””â”€â”€ session.py
    â””â”€â”€ domains/            # âœ¨ í•µì‹¬: ë„ë©”ì¸ë³„ ëª¨ë“ˆ
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ auth/           # ì¸ì¦ ë„ë©”ì¸
        â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”œâ”€â”€ controller.py   # API ì—”ë“œí¬ì¸íŠ¸ (ë¼ìš°í„°)
        â”‚   â”œâ”€â”€ schema/         # Pydantic ìŠ¤í‚¤ë§ˆ
        â”‚   â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”‚   â”œâ”€â”€ request.py  # ìš”ì²­ ìŠ¤í‚¤ë§ˆ
        â”‚   â”‚   â””â”€â”€ response.py # ì‘ë‹µ ìŠ¤í‚¤ë§ˆ
        â”‚   â””â”€â”€ service/        # ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§
        â”‚       â”œâ”€â”€ __init__.py
        â”‚       â”œâ”€â”€ kakao_service.py    # ì¹´ì¹´ì˜¤ OAuth ì„œë¹„ìŠ¤
        â”‚       â””â”€â”€ session_service.py  # ì„¸ì…˜ ê´€ë¦¬ ì„œë¹„ìŠ¤
        â”œâ”€â”€ users/          # ì‚¬ìš©ì ë„ë©”ì¸
        â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”œâ”€â”€ models.py       # User ì—”í‹°í‹° ëª¨ë¸
        â”‚   â”œâ”€â”€ schema.py       # User Pydantic ìŠ¤í‚¤ë§ˆ
        â”‚   â”œâ”€â”€ repository.py   # User ë°ì´í„° ì ‘ê·¼ ê³„ì¸µ
        â”‚   â””â”€â”€ service.py      # User ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§
        â”œâ”€â”€ documents/      # ë¬¸ì„œ ê´€ë¦¬ ë„ë©”ì¸
        â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”œâ”€â”€ models.py       # Document ì—”í‹°í‹° ëª¨ë¸
        â”‚   â”œâ”€â”€ schema.py       # Document Pydantic ìŠ¤í‚¤ë§ˆ
        â”‚   â”œâ”€â”€ repository.py   # Document ë°ì´í„° ì ‘ê·¼ ê³„ì¸µ
        â”‚   â”œâ”€â”€ service.py      # Document ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ (íŒŒì¼ ê²€ì¦, MinIO ì—…ë¡œë“œ, AI íƒœê¹…)
        â”‚   â””â”€â”€ controller.py   # Document API ì—”ë“œí¬ì¸íŠ¸
        â””â”€â”€ tags/           # íƒœê·¸ ë„ë©”ì¸
            â”œâ”€â”€ __init__.py
            â”œâ”€â”€ models.py       # Tag, DocumentTag ì—”í‹°í‹° ëª¨ë¸
            â”œâ”€â”€ schema.py       # Tag Pydantic ìŠ¤í‚¤ë§ˆ
            â”œâ”€â”€ repository.py   # Tag ë°ì´í„° ì ‘ê·¼ ê³„ì¸µ
            â””â”€â”€ service.py      # Tag ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§
```

---

## ğŸ› ï¸ ê¸°ìˆ  ìŠ¤íƒ

-   **Framework**: FastAPI
-   **Database**: PostgreSQL (SQLAlchemy ORM, Alembic)
-   **Cache**: Redis
-   **Search**: Elasticsearch
-   **Object Storage**: MinIO
-   **Data Validation**: Pydantic
-   **AI Frameworks**:
    -   LangChain, LangGraph (RAG íŒŒì´í”„ë¼ì¸)
    -   KeyBERT (í‚¤ì›Œë“œ ì¶”ì¶œ)
    -   Sentence Transformers (ì„ë² ë”©)
    -   OpenAI API (LLM)
-   **Async Runtime**: Uvicorn

---

## ğŸ ì‹œì‘í•˜ê¸° (Getting Started)

### 1. ë ˆí¬ì§€í† ë¦¬ í´ë¡  ë° ê°€ìƒ í™˜ê²½ ì„¤ì •

```bash
git clone https://github.com/Chaehyunli/Searchive-backend.git 
cd Searchive-backend
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
```

### 2. ì˜ì¡´ì„± ì„¤ì¹˜

```bash
pip install -r requirements.txt
```

### 3. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

`.env_example` íŒŒì¼ì„ ë³µì‚¬í•˜ì—¬ `.env` íŒŒì¼ì„ ìƒì„±í•˜ê³ , `Searchive-db` ìŠ¤íƒì˜ ì ‘ì† ì •ë³´ë¥¼ ì…ë ¥í•©ë‹ˆë‹¤.

```bash
cp .env_example .env
```

ê·¸ í›„ `.env` íŒŒì¼ì„ ì—´ì–´ ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´ ë° API í‚¤ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.

**í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜:**
- `DB_HOST`, `DB_PORT`, `DB_USER`, `DB_PASSWORD`, `DB_NAME`: PostgreSQL ì„¤ì •
- `REDIS_HOST`, `REDIS_PORT`: Redis ì„¤ì •
- `ELASTICSEARCH_HOST`, `ELASTICSEARCH_PORT`: Elasticsearch ì„¤ì •
- `MINIO_ENDPOINT`, `MINIO_ACCESS_KEY`, `MINIO_SECRET_KEY`: MinIO ì„¤ì •
- `OPENAI_API_KEY`: OpenAI API í‚¤ (LLM ì‚¬ìš©)
- `KEYWORD_EXTRACTION_COUNT`: ìë™ íƒœê·¸ ì¶”ì¶œ ê°œìˆ˜ (ê¸°ë³¸ê°’: 3)
- `KAKAO_CLIENT_ID`, `KAKAO_CLIENT_SECRET`: ì¹´ì¹´ì˜¤ OAuth ì„¤ì •

### 4. DB ì¸í”„ë¼ ì‹¤í–‰

`Searchive-db` ë ˆí¬ì§€í† ë¦¬ì—ì„œ `docker compose up -d`ë¥¼ ì‹¤í–‰í•˜ì—¬ ëª¨ë“  ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì¤€ë¹„ì‹œí‚µë‹ˆë‹¤.

### 5. ë°ì´í„°ë² ì´ìŠ¤ ë§ˆì´ê·¸ë ˆì´ì…˜

ë°±ì—”ë“œ ì„œë²„ë¥¼ ì‹¤í–‰í•˜ê¸° ì „ì—, ì•„ë˜ ëª…ë ¹ì–´ë¡œ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

```bash
# Alembic ì´ˆê¸°í™” (ìµœì´ˆ 1íšŒë§Œ)
alembic init alembic

# ë§ˆì´ê·¸ë ˆì´ì…˜ íŒŒì¼ ìƒì„±
alembic revision --autogenerate -m "Initial migration"

# ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰
alembic upgrade head
```

### 6. ì„œë²„ ì‹¤í–‰

```bash
# ê°œë°œ ëª¨ë“œ (ìë™ ë¦¬ë¡œë“œ)
.\venv\Scripts\activate
uvicorn src.main:app --reload --host 0.0.0.0 --port 8000

# ë˜ëŠ”
# python src/main.py
```

ì„œë²„ê°€ ì‹¤í–‰ë˜ë©´ ë‹¤ìŒ URLì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

-   API ì„œë²„: http://localhost:8000
-   API ë¬¸ì„œ (Swagger): http://localhost:8000/docs
-   API ë¬¸ì„œ (ReDoc): http://localhost:8000/redoc

---

## ğŸ“„ Documents API (ë¬¸ì„œ ê´€ë¦¬)

Documents ë„ë©”ì¸ì€ ì‚¬ìš©ìì˜ íŒŒì¼ ì—…ë¡œë“œ, ì¡°íšŒ, ì‚­ì œ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

### API ì—”ë“œí¬ì¸íŠ¸

#### 1. ë¬¸ì„œ ì—…ë¡œë“œ (POST /api/v1/documents/upload)
ì‚¬ìš©ìê°€ ë¬¸ì„œë¥¼ MinIOì— ì—…ë¡œë“œí•˜ê³  ë©”íƒ€ë°ì´í„°ë¥¼ PostgreSQLì— ì €ì¥í•©ë‹ˆë‹¤. **AI ê¸°ë°˜ ìë™ íƒœê·¸ ìƒì„±** ê¸°ëŠ¥ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

**ìš”ì²­:**
- Method: `POST`
- Content-Type: `multipart/form-data`
- Body: `file` (íŒŒì¼)
- Headers: `Cookie: session_id` (ì¸ì¦ í•„ìš”)

**í—ˆìš©ëœ íŒŒì¼ í˜•ì‹:**
- PDF (`.pdf`)
- í…ìŠ¤íŠ¸ (`.txt`)
- Excel (`.xlsx`, `.xls`)
- Word (`.doc`, `.docx`)
- PowerPoint (`.ppt`, `.pptx`)

**ì‘ë‹µ (201 Created):**
```json
{
  "document_id": 101,
  "user_id": 1,
  "original_filename": "my_report.pdf",
  "storage_path": "1/a1b2c3d4-...-uuid.pdf",
  "file_type": "application/pdf",
  "file_size_kb": 1234,
  "uploaded_at": "2025-10-08T15:30:00Z",
  "updated_at": "2025-10-08T15:30:00Z",
  "tags": [
    {"tag_id": 1, "name": "machine learning"},
    {"tag_id": 2, "name": "deep learning"},
    {"tag_id": 3, "name": "neural network"}
  ],
  "extraction_method": "keybert"
}
```

**AI ìë™ íƒœê¹…:**
- ë¬¸ì„œ ì—…ë¡œë“œ ì‹œ AIê°€ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ìë™ìœ¼ë¡œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤
- ì¶”ì¶œ ë°©ë²•: KeyBERT (ê¸°ë³¸) ë˜ëŠ” Elasticsearch TF-IDF (ë°±ì—…)
- ì¶”ì¶œëœ í‚¤ì›Œë“œëŠ” `tags` í…Œì´ë¸”ì— ì €ì¥ë˜ê³ , `document_tags` ì—°ê²° í…Œì´ë¸”ì„ í†µí•´ ë¬¸ì„œì™€ ì—°ê²°ë©ë‹ˆë‹¤

#### 2. ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ (GET /api/v1/documents)
í˜„ì¬ ë¡œê·¸ì¸ëœ ì‚¬ìš©ìì˜ ëª¨ë“  ë¬¸ì„œ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤. ê° ë¬¸ì„œì— ì—°ê²°ëœ íƒœê·¸ ì •ë³´ë„ í•¨ê»˜ ë°˜í™˜ë©ë‹ˆë‹¤.

**ì‘ë‹µ (200 OK):**
```json
[
  {
    "document_id": 101,
    "original_filename": "report.pdf",
    "file_type": "application/pdf",
    "file_size_kb": 1234,
    "uploaded_at": "2025-10-08T15:30:00Z",
    "updated_at": "2025-10-08T15:30:00Z",
    "tags": [
      {"tag_id": 1, "name": "machine learning"},
      {"tag_id": 2, "name": "deep learning"}
    ]
  }
]
```

#### 3. ë¬¸ì„œ ìƒì„¸ ì¡°íšŒ (GET /api/v1/documents/{document_id})
íŠ¹ì • ë¬¸ì„œì˜ ìƒì„¸ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤. (ê¶Œí•œ ê²€ì¦ í¬í•¨)

**ì‘ë‹µ (200 OK):**
```json
{
  "document_id": 101,
  "user_id": 1,
  "original_filename": "my_report.pdf",
  "storage_path": "1/a1b2c3d4-...-uuid.pdf",
  "file_type": "application/pdf",
  "file_size_kb": 1234,
  "uploaded_at": "2025-10-08T15:30:00Z",
  "updated_at": "2025-10-08T15:30:00Z",
  "tags": [
    {"tag_id": 1, "name": "machine learning"},
    {"tag_id": 2, "name": "deep learning"}
  ]
}
```

#### 4. ë¬¸ì„œ ì‚­ì œ (DELETE /api/v1/documents/{document_id})
ë¬¸ì„œë¥¼ MinIOì™€ PostgreSQLì—ì„œ ì™„ì „íˆ ì‚­ì œí•©ë‹ˆë‹¤. (ê´€ë ¨ íƒœê·¸ë„ CASCADE ì‚­ì œ)

**ì‘ë‹µ (200 OK):**
```json
{
  "message": "ë¬¸ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.",
  "document_id": 101
}
```

### íŒŒì¼ ì €ì¥ êµ¬ì¡°

**MinIO ë²„í‚· êµ¬ì¡°:**
```
user-documents/
  â”œâ”€â”€ 1/                          # user_id=1ì˜ í´ë”
  â”‚   â”œâ”€â”€ a1b2c3d4-uuid.pdf
  â”‚   â”œâ”€â”€ e5f6g7h8-uuid.docx
  â”‚   â””â”€â”€ i9j0k1l2-uuid.xlsx
  â”œâ”€â”€ 2/                          # user_id=2ì˜ í´ë”
  â”‚   â”œâ”€â”€ m3n4o5p6-uuid.pdf
  â”‚   â””â”€â”€ q7r8s9t0-uuid.txt
```

- ê° ì‚¬ìš©ìëŠ” `user_id` í´ë”ë¡œ ê²©ë¦¬ë©ë‹ˆë‹¤.
- íŒŒì¼ëª…ì€ **UUID(ê³ ìœ  ëœë¤ê°’)**ë¡œ ì €ì¥ë˜ì–´ ì¶©ëŒê³¼ ì¶”ì¸¡ì„ ë°©ì§€í•©ë‹ˆë‹¤.
- ì›ë³¸ íŒŒì¼ëª…ì€ PostgreSQLì˜ `original_filename` ì»¬ëŸ¼ì— ì €ì¥ë©ë‹ˆë‹¤.

### ë³´ì•ˆ ë° ê¶Œí•œ

- **ì¸ì¦ í•„ìˆ˜:** ëª¨ë“  APIëŠ” `get_current_user_id` ì˜ì¡´ì„±ì„ í†µí•´ ë¡œê·¸ì¸ëœ ì‚¬ìš©ìë§Œ ì ‘ê·¼ ê°€ëŠ¥í•©ë‹ˆë‹¤.
- **ê¶Œí•œ ê²€ì¦:** ì‚¬ìš©ìëŠ” ìì‹ ì´ ì—…ë¡œë“œí•œ ë¬¸ì„œë§Œ ì¡°íšŒ/ì‚­ì œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- **íŒŒì¼ í˜•ì‹ ê²€ì¦:** MIME íƒ€ì… ê¸°ë°˜ìœ¼ë¡œ í—ˆìš©ëœ í˜•ì‹ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤.

### ì•„í‚¤í…ì²˜ (ê³„ì¸µ ë¶„ë¦¬)

Documents ë„ë©”ì¸ì€ SOLID ì›ì¹™ì„ ë”°ë¼ ê³„ì¸µì´ ë¶„ë¦¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤:

1. **Controller (`controller.py`)**: HTTP ìš”ì²­/ì‘ë‹µ ì²˜ë¦¬, ì¸ì¦ ê²€ì¦
2. **Service (`service.py`)**: ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ (íŒŒì¼ ê²€ì¦, MinIO ì—…ë¡œë“œ, ì—ëŸ¬ ì²˜ë¦¬)
3. **Repository (`repository.py`)**: DB CRUD ì—°ì‚° (N+1 ë¬¸ì œ ë°©ì§€)
4. **Schema (`schema.py`)**: Pydantic ëª¨ë¸ (ìš”ì²­/ì‘ë‹µ ê²€ì¦)
5. **Models (`models.py`)**: SQLAlchemy ORM ëª¨ë¸ (DB í…Œì´ë¸”)

---

## ğŸ·ï¸ Tags & DocumentTags (íƒœê·¸ ì‹œìŠ¤í…œ)

Tags ë„ë©”ì¸ì€ ë¬¸ì„œ ë¶„ë¥˜ì™€ ê²€ìƒ‰ì„ ìœ„í•œ íƒœê·¸ ê´€ë¦¬ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. **ë‹¤ëŒ€ë‹¤(Many-to-Many) ê´€ê³„**ë¥¼ ì§€ì›í•˜ì—¬ í•˜ë‚˜ì˜ ë¬¸ì„œì— ì—¬ëŸ¬ íƒœê·¸ë¥¼ ì—°ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¡°

#### 1. `tags` í…Œì´ë¸”
```sql
CREATE TABLE tags (
    tag_id BIGSERIAL PRIMARY KEY,
    name VARCHAR(100) UNIQUE NOT NULL,
    created_at TIMESTAMP DEFAULT NOW()
);
```

- **tag_id**: íƒœê·¸ ê³ ìœ  ID (ìë™ ì¦ê°€)
- **name**: íƒœê·¸ ì´ë¦„ (ì¤‘ë³µ ë¶ˆê°€, ì¸ë±ìŠ¤)
- **created_at**: íƒœê·¸ ìƒì„± ì¼ì‹œ

#### 2. `document_tags` í…Œì´ë¸” (ì—°ê²° í…Œì´ë¸”)
```sql
CREATE TABLE document_tags (
    document_id BIGINT REFERENCES documents(document_id) ON DELETE CASCADE,
    tag_id BIGINT REFERENCES tags(tag_id) ON DELETE CASCADE,
    created_at TIMESTAMP DEFAULT NOW(),
    PRIMARY KEY (document_id, tag_id)
);
```

- **document_id**: ë¬¸ì„œ ID (ì™¸ë˜ í‚¤, CASCADE ì‚­ì œ)
- **tag_id**: íƒœê·¸ ID (ì™¸ë˜ í‚¤, CASCADE ì‚­ì œ)
- **created_at**: ì—°ê²° ìƒì„± ì¼ì‹œ
- **ë³µí•© ê¸°ë³¸ í‚¤**: (document_id, tag_id) - ì¤‘ë³µ ì—°ê²° ë°©ì§€

### AI ê¸°ë°˜ ìë™ íƒœê¹… (Keyword Extraction)

ë¬¸ì„œ ì—…ë¡œë“œ ì‹œ AIê°€ ìë™ìœ¼ë¡œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ê³  íƒœê·¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì´ ì‹œìŠ¤í…œì€ **í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì¶œ ì „ëµ**ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ì–‘ì— ë”°ë¼ ìµœì ì˜ ë°©ë²•ì„ ì„ íƒí•©ë‹ˆë‹¤.

#### í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì¶œ ì „ëµ (Cold Start â†’ Normal)

**1. Cold Start ëª¨ë“œ (ë¬¸ì„œ ìˆ˜ < ì„ê³„ê°’)**
- **ì‚¬ìš© ì¡°ê±´**: Elasticsearch ìƒ‰ì¸ ë¬¸ì„œ ìˆ˜ < 10ê°œ (ê¸°ë³¸ ì„ê³„ê°’)
- **ì¶”ì¶œ ë°©ë²•**: KeyBERT (BERT ê¸°ë°˜ ì„ë² ë”©)
- **ì¥ì **:
  - ë¬¸ì„œ ê°„ ë¹„êµ ë°ì´í„°ê°€ ë¶€ì¡±í•´ë„ ë‹¨ì¼ ë¬¸ì„œì—ì„œ ì˜ë¯¸ ìˆëŠ” í‚¤ì›Œë“œ ì¶”ì¶œ
  - ë¬¸ë§¥ì„ ê³ ë ¤í•œ ì˜ë¯¸ë¡ ì  í‚¤ì›Œë“œ ì¶”ì¶œ
- **ì¶”ì¶œ ê³¼ì •**:
  ```python
  # KeyBERT ëª¨ë¸ ë¡œë“œ (Lazy Loading)
  model = KeyBERT()

  # í‚¤ì›Œë“œ ì¶”ì¶œ
  keywords = model.extract_keywords(
      text,
      keyphrase_ngram_range=(1, 2),  # 1~2 ë‹¨ì–´ êµ¬ë¬¸ê¹Œì§€ í‚¤ì›Œë“œë¡œ ê³ ë ¤
      stop_words='english',          # ì˜ì–´ ë¶ˆìš©ì–´ ì œê±° (is, a, the ë“±)
      top_n=3,                        # ìƒìœ„ 3ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ
      use_maxsum=True,                # ë‹¤ì–‘ì„± ì¦ê°€ (í‚¤ì›Œë“œ ì¤‘ë³µ ë°©ì§€)
      nr_candidates=20                # ë‚´ë¶€ í›„ë³´ í‚¤ì›Œë“œ 20ê°œ ìƒì„± í›„ í•„í„°ë§
  )
  # ê²°ê³¼: [("machine learning", 0.85), ("deep learning", 0.78), ...]
  ```

**2. Normal ëª¨ë“œ (ë¬¸ì„œ ìˆ˜ >= ì„ê³„ê°’)**
- **ì‚¬ìš© ì¡°ê±´**: Elasticsearch ìƒ‰ì¸ ë¬¸ì„œ ìˆ˜ >= 10ê°œ
- **ì¶”ì¶œ ë°©ë²•**: Elasticsearch TF-IDF (Term Vectors API)
- **ì¥ì **:
  - ì „ì²´ ë¬¸ì„œ ì»¬ë ‰ì…˜ê³¼ ë¹„êµí•˜ì—¬ ìƒëŒ€ì  ì¤‘ìš”ë„ ê³„ì‚°
  - ë‹¤ë¥¸ ë¬¸ì„œì—ëŠ” ì—†ì§€ë§Œ í•´ë‹¹ ë¬¸ì„œì—ì„œ ì¤‘ìš”í•œ í‚¤ì›Œë“œ ì¶”ì¶œ
- **ì¶”ì¶œ ê³¼ì •**:
  ```python
  # Term Vectors APIë¡œ TF-IDF ê³„ì‚°
  tv_response = await client.termvectors(
      index="documents",
      id=str(document_id),
      fields=["content"],
      term_statistics=True,
      field_statistics=True
  )

  # TF-IDF ì ìˆ˜ ê³„ì‚°
  for term, term_info in terms.items():
      tf = term_info["term_freq"]        # í•´ë‹¹ ë¬¸ì„œì—ì„œ ë‹¨ì–´ ë¹ˆë„
      df = term_info["doc_freq"]         # ì „ì²´ ë¬¸ì„œì—ì„œ ë‹¨ì–´ ë¹ˆë„
      idf = log((total_docs + 1) / (df + 1)) + 1
      tfidf = tf * idf

  # ìƒìœ„ Nê°œ ì¶”ì¶œ (ì ìˆ˜ ê¸°ì¤€ ì •ë ¬)
  keywords = sorted(scores, reverse=True)[:3]
  ```

#### ì „ì²´ ì¶”ì¶œ í”„ë¡œì„¸ìŠ¤ (9ë‹¨ê³„)

```
[1] ì‚¬ìš©ì íŒŒì¼ ì—…ë¡œë“œ (Controller)
    â†“
[2] íŒŒì¼ í˜•ì‹ ê²€ì¦ (Service)
    - MIME íƒ€ì… ê²€ì¦ (PDF, DOCX, PPTX, XLSX, TXTë§Œ í—ˆìš©)
    â†“
[3] ê³ ìœ  ê²½ë¡œ ìƒì„± (Service)
    - UUID ê¸°ë°˜ íŒŒì¼ëª… ìƒì„±: {user_id}/{uuid}.í™•ì¥ì
    - ì˜ˆ: 123/a1b2c3d4-e5f6-7890-abcd-ef1234567890.pdf
    â†“
[4] MinIO ì—…ë¡œë“œ (MinIO Client)
    - ë²„í‚·: user-documents
    - ê°ì²´ ìŠ¤í† ë¦¬ì§€ì— ì‹¤ì œ íŒŒì¼ ì €ì¥
    â†“
[5] PostgreSQL ë©”íƒ€ë°ì´í„° ì €ì¥ (Repository)
    - í…Œì´ë¸”: documents
    - ì»¬ëŸ¼: document_id, user_id, original_filename, storage_path, file_type, file_size_kb
    â†“
[6] í…ìŠ¤íŠ¸ ì¶”ì¶œ (TextExtractor)
    - PDF â†’ pypdf ë¼ì´ë¸ŒëŸ¬ë¦¬
    - DOCX â†’ python-docx ë¼ì´ë¸ŒëŸ¬ë¦¬
    - XLSX â†’ openpyxl ë¼ì´ë¸ŒëŸ¬ë¦¬
    - PPTX â†’ python-pptx ë¼ì´ë¸ŒëŸ¬ë¦¬
    - TXT â†’ UTF-8/CP949 ë””ì½”ë”©
    â†“
[7] Elasticsearch ìƒ‰ì¸ (Elasticsearch Client)
    - ì¸ë±ìŠ¤: documents
    - í•„ë“œ: document_id, user_id, content, filename, file_type, uploaded_at
    - í•œêµ­ì–´ ë¶„ì„ê¸° ì ìš© (korean_analyzer)
    â†“
[8] í•˜ì´ë¸Œë¦¬ë“œ í‚¤ì›Œë“œ ì¶”ì¶œ (Keyword Extraction Service)
    â”œâ”€ ë¬¸ì„œ ìˆ˜ í™•ì¸: await elasticsearch_client.get_document_count()
    â”œâ”€ ë¬¸ì„œ < 10: KeyBERT ì‚¬ìš© (Cold Start)
    â””â”€ ë¬¸ì„œ >= 10: Elasticsearch TF-IDF ì‚¬ìš© (Normal)
    â†“
[9] íƒœê·¸ ìƒì„± ë° ë¬¸ì„œ ì—°ê²° (TagService)
    - tags í…Œì´ë¸”: Get-or-Create íŒ¨í„´ (ì¤‘ë³µ ë°©ì§€)
    - document_tags í…Œì´ë¸”: Bulk Insert (N+1 ë°©ì§€)
```

#### ì½”ë“œ íë¦„ (íŒŒì¼ â†’ í•¨ìˆ˜ ë‹¨ìœ„)

**Controller Layer** (`documents/controller.py:34-78`)
```python
@router.post("/upload")
async def upload_document(file: UploadFile, user_id: int, document_service: DocumentService):
    # 1. Service í˜¸ì¶œ
    document, tags, extraction_method = await document_service.upload_document(
        user_id=user_id,
        file=file
    )

    # 2. ì‘ë‹µ ìƒì„±
    return DocumentUploadResponse(
        document_id=document.document_id,
        tags=[TagSchema(tag_id=tag.tag_id, name=tag.name) for tag in tags],
        extraction_method=extraction_method  # "keybert" or "elasticsearch"
    )
```

**Service Layer** (`documents/service.py:48-158`)
```python
async def upload_document(user_id: int, file: UploadFile):
    # Step 1-3: ê²€ì¦ ë° ê²½ë¡œ ìƒì„±
    unique_filename = f"{uuid.uuid4()}{file.suffix}"  # UUID ìƒì„±
    storage_path = f"{user_id}/{unique_filename}"

    # Step 4: MinIO ì—…ë¡œë“œ
    minio_client.upload_file(storage_path, file_data, file_size, content_type)

    # Step 5: PostgreSQL ì €ì¥
    document = await document_repository.create(
        user_id, original_filename, storage_path, file_type, file_size_kb
    )

    # Step 6: í…ìŠ¤íŠ¸ ì¶”ì¶œ
    extracted_text = text_extractor.extract_text_from_bytes(
        file_data, file_type, filename
    )

    # Step 7: Elasticsearch ìƒ‰ì¸
    await elasticsearch_client.index_document(
        document_id=document.document_id,
        user_id=user_id,
        content=extracted_text,
        filename=filename,
        file_type=file_type,
        uploaded_at=document.uploaded_at.isoformat()
    )

    # Step 8: í•˜ì´ë¸Œë¦¬ë“œ í‚¤ì›Œë“œ ì¶”ì¶œ
    keywords, extraction_method = await keyword_extraction_service.extract_keywords(
        text=extracted_text,
        document_id=document.document_id
    )

    # Step 9: íƒœê·¸ ìƒì„± ë° ì—°ê²°
    tags = await tag_service.attach_tags_to_document(
        document_id=document.document_id,
        tag_names=keywords
    )

    return document, tags, extraction_method
```

**TextExtractor** (`core/text_extractor.py:14-56`)
```python
@staticmethod
def extract_text_from_bytes(file_data: bytes, file_type: str, filename: str):
    # íŒŒì¼ íƒ€ì…ì— ë”°ë¼ ì ì ˆí•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©
    if file_type == "application/pdf":
        return TextExtractor._extract_from_pdf(file_data)
    elif file_type == "text/plain":
        return TextExtractor._extract_from_txt(file_data)
    elif "wordprocessing" in file_type:  # DOCX
        return TextExtractor._extract_from_docx(file_data)
    elif "spreadsheet" in file_type:     # XLSX
        return TextExtractor._extract_from_excel(file_data)
    elif "presentation" in file_type:    # PPTX
        return TextExtractor._extract_from_pptx(file_data)

@staticmethod
def _extract_from_pdf(file_data: bytes):
    """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (pypdf ë¼ì´ë¸ŒëŸ¬ë¦¬)"""
    import pypdf
    pdf_reader = pypdf.PdfReader(BytesIO(file_data))
    text_parts = [page.extract_text() for page in pdf_reader.pages]
    return "\n".join(text_parts)
```

**Elasticsearch Client** (`core/elasticsearch_client.py:83-127`)
```python
async def index_document(document_id, user_id, content, filename, file_type, uploaded_at):
    """Elasticsearchì— ë¬¸ì„œ ìƒ‰ì¸"""
    doc_body = {
        "document_id": document_id,
        "user_id": user_id,
        "content": content,              # ì¶”ì¶œëœ í…ìŠ¤íŠ¸
        "filename": filename,
        "file_type": file_type,
        "uploaded_at": uploaded_at
    }

    await self.client.index(
        index="documents",
        id=str(document_id),
        document=doc_body
    )

async def extract_significant_terms(document_id, size=3):
    """Elasticsearch TF-IDF ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    tv_response = await self.client.termvectors(
        index="documents",
        id=str(document_id),
        fields=["content"],
        term_statistics=True,
        field_statistics=True
    )

    # TF-IDF ê³„ì‚°
    terms = tv_response["term_vectors"]["content"]["terms"]
    term_scores = []
    for term, term_info in terms.items():
        tf = term_info["term_freq"]
        df = term_info["doc_freq"]
        total_docs = tv_response["term_vectors"]["content"]["field_statistics"]["doc_count"]
        idf = math.log((total_docs + 1) / (df + 1)) + 1
        tfidf = tf * idf

        if 2 <= len(term) <= 30:  # ë‹¨ì–´ ê¸¸ì´ í•„í„°
            term_scores.append((term, tfidf))

    # ìƒìœ„ Nê°œ ë°˜í™˜
    term_scores.sort(key=lambda x: x[1], reverse=True)
    return [term for term, score in term_scores[:size]]
```

**Keyword Extraction Service** (`core/keyword_extraction.py:142-178`)
```python
class HybridKeywordExtractionService:
    """í•˜ì´ë¸Œë¦¬ë“œ í‚¤ì›Œë“œ ì¶”ì¶œ ì„œë¹„ìŠ¤ (Orchestrator)"""

    async def extract_keywords(text, document_id):
        # 1. í˜„ì¬ Elasticsearch ë¬¸ì„œ ìˆ˜ í™•ì¸
        document_count = await elasticsearch_client.get_document_count()

        # 2. ì„ê³„ê°’ ê¸°ë°˜ ì „ëµ ì„ íƒ
        if document_count < self.threshold:  # ê¸°ë³¸ê°’: 10
            # Cold Start: KeyBERT ì‚¬ìš©
            keywords = await keybert_extractor.extract_keywords(text)
            method = "keybert"
        else:
            # Normal: Elasticsearch TF-IDF ì‚¬ìš©
            keywords = await elasticsearch_extractor.extract_keywords(text, document_id)
            method = "elasticsearch"

        # 3. í‚¤ì›Œë“œ ì •ê·œí™” (ì†Œë¬¸ì, ì¤‘ë³µ ì œê±°)
        keywords = list(set(kw.strip().lower() for kw in keywords))

        return keywords, method

class KeyBERTExtractor(KeywordExtractor):
    """KeyBERT ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œê¸°"""

    async def extract_keywords(text, document_id=None):
        # KeyBERT ëª¨ë¸ ë¡œë“œ (ìµœì´ˆ 1íšŒ)
        if self.model is None:
            from keybert import KeyBERT
            self.model = KeyBERT()

        # í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords_with_scores = self.model.extract_keywords(
            text,
            keyphrase_ngram_range=(1, 2),
            stop_words='english',
            top_n=3,
            use_maxsum=True,
            nr_candidates=20
        )

        return [kw[0] for kw in keywords_with_scores]

class ElasticsearchExtractor(KeywordExtractor):
    """Elasticsearch TF-IDF ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œê¸°"""

    async def extract_keywords(text, document_id):
        keywords = await elasticsearch_client.extract_significant_terms(
            document_id=document_id,
            size=3
        )
        return keywords
```

**Tag Service** (`tags/service.py:59-87`)
```python
async def attach_tags_to_document(document_id, tag_names):
    """íƒœê·¸ ìƒì„± ë° ë¬¸ì„œ ì—°ê²°"""
    # 1. íƒœê·¸ ì¡°íšŒ ë˜ëŠ” ìƒì„± (Get-or-Create, N+1 ë°©ì§€)
    tags = await self.get_or_create_tags(tag_names)

    # 2. ë¬¸ì„œ-íƒœê·¸ ì—°ê²° ìƒì„± (Bulk Insert)
    tag_ids = [tag.tag_id for tag in tags]
    await self.document_tag_repository.bulk_create(document_id, tag_ids)

    return tags
```

**Tag Repository** (`tags/repository.py:121-147`)
```python
async def bulk_get_or_create(names):
    """ì—¬ëŸ¬ íƒœê·¸ë¥¼ í•œ ë²ˆì— ì¡°íšŒ ë˜ëŠ” ìƒì„± (N+1 ë°©ì§€)"""
    # 1. ê¸°ì¡´ íƒœê·¸ ì¡°íšŒ (í•œ ë²ˆì˜ ì¿¼ë¦¬)
    existing_tags = await find_all_by_names(names)
    existing_tag_names = {tag.name for tag in existing_tags}

    # 2. ì‹ ê·œ íƒœê·¸ í•„í„°ë§
    new_tag_names = [name for name in names if name not in existing_tag_names]

    # 3. ì‹ ê·œ íƒœê·¸ ìƒì„± (Bulk Insert)
    new_tags = await bulk_create(new_tag_names) if new_tag_names else []

    # 4. ê¸°ì¡´ + ì‹ ê·œ ë°˜í™˜
    return existing_tags + new_tags

async def bulk_create(names):
    """ì—¬ëŸ¬ íƒœê·¸ë¥¼ í•œ ë²ˆì— ìƒì„±"""
    tags = [Tag(name=name) for name in names]
    self.db.add_all(tags)
    await self.db.commit()
    return tags
```

**Document Tag Repository** (`tags/repository.py:182-207`)
```python
async def bulk_create(document_id, tag_ids):
    """ë¬¸ì„œ-íƒœê·¸ ì—°ê²° ìƒì„± (Bulk Insert)"""
    document_tags = [
        DocumentTag(document_id=document_id, tag_id=tag_id)
        for tag_id in tag_ids
    ]
    self.db.add_all(document_tags)
    await self.db.commit()
    return document_tags
```

### íƒœê·¸ ê´€ë¦¬ ê¸°ëŠ¥

#### 1. Get-or-Create íŒ¨í„´
- íƒœê·¸ê°€ ì´ë¯¸ ì¡´ì¬í•˜ë©´ ì¬ì‚¬ìš©, ì—†ìœ¼ë©´ ì‹ ê·œ ìƒì„±
- ì¤‘ë³µ íƒœê·¸ ë°©ì§€ ë° ë°ì´í„° ì¼ê´€ì„± ìœ ì§€

```python
# ì˜ˆì‹œ: "machine learning" íƒœê·¸
tag = await tag_service.get_or_create_tag("machine learning")
```

#### 2. N+1 ë¬¸ì œ ë°©ì§€
- ì—¬ëŸ¬ íƒœê·¸ë¥¼ í•œ ë²ˆì˜ ì¿¼ë¦¬ë¡œ ì¡°íšŒ/ìƒì„±
- `bulk_get_or_create()` ë©”ì„œë“œ ì‚¬ìš©

```python
# ì˜ˆì‹œ: 3ê°œ íƒœê·¸ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬
tags = await tag_service.get_or_create_tags(
    ["machine learning", "deep learning", "neural network"]
)
```

#### 3. CASCADE ì‚­ì œ
- ë¬¸ì„œ ì‚­ì œ ì‹œ ì—°ê²°ëœ `document_tags` ìë™ ì‚­ì œ
- íƒœê·¸ëŠ” ë‹¤ë¥¸ ë¬¸ì„œì—ì„œë„ ì‚¬ìš©ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìœ ì§€

### ì•„í‚¤í…ì²˜ (ê³„ì¸µ ë¶„ë¦¬)

Tags ë„ë©”ì¸ë„ SOLID ì›ì¹™ì„ ë”°ë¼ ê³„ì¸µì´ ë¶„ë¦¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤:

1. **Models (`models.py`)**:
   - `Tag`: íƒœê·¸ ì—”í‹°í‹°
   - `DocumentTag`: ë¬¸ì„œ-íƒœê·¸ ì—°ê²° ì—”í‹°í‹°

2. **Repository (`repository.py`)**:
   - `TagRepository`: íƒœê·¸ CRUD ì—°ì‚°
   - `DocumentTagRepository`: ë¬¸ì„œ-íƒœê·¸ ì—°ê²° CRUD ì—°ì‚°

3. **Service (`service.py`)**:
   - `TagService`: íƒœê·¸ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ (Get-or-Create, ë¬¸ì„œ ì—°ê²°)

4. **Schema (`schema.py`)**:
   - `TagResponse`: íƒœê·¸ ì‘ë‹µ ìŠ¤í‚¤ë§ˆ
   - `DocumentTagResponse`: ë¬¸ì„œ-íƒœê·¸ ì—°ê²° ì‘ë‹µ ìŠ¤í‚¤ë§ˆ
   - `ExtractedKeywordsResponse`: í‚¤ì›Œë“œ ì¶”ì¶œ ê²°ê³¼ ìŠ¤í‚¤ë§ˆ

### ì„±ëŠ¥ ìµœì í™”

- **ì¸ë±ìŠ¤**: `tags.name` ì»¬ëŸ¼ì— ì¸ë±ìŠ¤ ì„¤ì • (ë¹ ë¥¸ ì¡°íšŒ)
- **Bulk Operations**: ì—¬ëŸ¬ íƒœê·¸ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬í•˜ì—¬ DB ì¿¼ë¦¬ ìµœì†Œí™”
- **Eager Loading**: `selectinload()`ë¥¼ ì‚¬ìš©í•˜ì—¬ N+1 ë¬¸ì œ ë°©ì§€
- **íŠ¸ëœì­ì…˜**: íƒœê·¸ ìƒì„±/ì—°ê²°ì„ í•˜ë‚˜ì˜ íŠ¸ëœì­ì…˜ìœ¼ë¡œ ì²˜ë¦¬

---

## ğŸ—„ï¸ MinIO (ê°ì²´ ìŠ¤í† ë¦¬ì§€)

MinIOëŠ” Amazon S3 í˜¸í™˜ APIë¥¼ ì œê³µí•˜ëŠ” ì˜¤í”ˆì†ŒìŠ¤ ê°ì²´ ìŠ¤í† ë¦¬ì§€ì…ë‹ˆë‹¤. Searchiveì—ì„œëŠ” ì‚¬ìš©ì ì—…ë¡œë“œ ë¬¸ì„œë¥¼ ì €ì¥í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.

### MinIO êµ¬ì¡°

**ë²„í‚· êµ¬ì¡°:**
```
user-documents/              # ë²„í‚· ì´ë¦„
  â”œâ”€â”€ 1/                     # user_id=1
  â”‚   â”œâ”€â”€ a1b2c3d4-uuid.pdf
  â”‚   â”œâ”€â”€ e5f6g7h8-uuid.docx
  â”‚   â””â”€â”€ i9j0k1l2-uuid.xlsx
  â”œâ”€â”€ 2/                     # user_id=2
  â”‚   â”œâ”€â”€ m3n4o5p6-uuid.pdf
  â”‚   â””â”€â”€ q7r8s9t0-uuid.txt
  â””â”€â”€ 3/                     # user_id=3
      â””â”€â”€ u1v2w3x4-uuid.pptx
```

### MinIO Client êµ¬í˜„ (`core/minio_client.py`)

**ì£¼ìš” ê¸°ëŠ¥:**

#### 1. íŒŒì¼ ì—…ë¡œë“œ
```python
def upload_file(file_path: str, file_data: BytesIO, file_size: int, content_type: str):
    """
    MinIOì— íŒŒì¼ ì—…ë¡œë“œ

    Args:
        file_path: ì €ì¥ ê²½ë¡œ (ì˜ˆ: "1/uuid.pdf")
        file_data: íŒŒì¼ ë°”ì´íŠ¸ ìŠ¤íŠ¸ë¦¼
        file_size: íŒŒì¼ í¬ê¸° (bytes)
        content_type: MIME íƒ€ì… (ì˜ˆ: "application/pdf")
    """
    client.put_object(
        bucket_name=BUCKET_NAME,
        object_name=file_path,
        data=file_data,
        length=file_size,
        content_type=content_type
    )
```

#### 2. íŒŒì¼ ë‹¤ìš´ë¡œë“œ
```python
def download_file(file_path: str) -> bytes:
    """
    MinIOì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ

    Returns:
        íŒŒì¼ ë°”ì´íŠ¸ ë°ì´í„°
    """
    response = client.get_object(
        bucket_name=BUCKET_NAME,
        object_name=file_path
    )
    return response.read()
```

#### 3. íŒŒì¼ ì‚­ì œ
```python
def delete_file(file_path: str):
    """
    MinIOì—ì„œ íŒŒì¼ ì‚­ì œ
    """
    client.remove_object(
        bucket_name=BUCKET_NAME,
        object_name=file_path
    )
```

### MinIO ì¥ì 

1. **í™•ì¥ì„±**: ìˆ˜í‰ í™•ì¥ìœ¼ë¡œ ëŒ€ìš©ëŸ‰ íŒŒì¼ ì €ì¥ ê°€ëŠ¥
2. **ë¹„ìš© íš¨ìœ¨**: ì˜¤í”ˆì†ŒìŠ¤ë¡œ ë¬´ë£Œ ì‚¬ìš© ê°€ëŠ¥
3. **S3 í˜¸í™˜**: AWS S3 APIì™€ í˜¸í™˜ë˜ì–´ ë§ˆì´ê·¸ë ˆì´ì…˜ ìš©ì´
4. **ë³´ì•ˆ**: ì ‘ê·¼ ì œì–´ ë° ì•”í˜¸í™” ì§€ì›
5. **ì„±ëŠ¥**: ë†’ì€ ì²˜ë¦¬ëŸ‰ê³¼ ë‚®ì€ ì§€ì—°ì‹œê°„

### í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

```bash
MINIO_ENDPOINT=localhost:9000
MINIO_ACCESS_KEY=minioadmin
MINIO_SECRET_KEY=minioadmin
MINIO_BUCKET_NAME=user-documents
MINIO_SECURE=False  # HTTPS ì‚¬ìš© ì—¬ë¶€
```

---

## ğŸ” Elasticsearch (ê²€ìƒ‰ ì—”ì§„)

ElasticsearchëŠ” ë¶„ì‚°í˜• ê²€ìƒ‰ ë° ë¶„ì„ ì—”ì§„ì…ë‹ˆë‹¤. Searchiveì—ì„œëŠ” ë¬¸ì„œ ì „ë¬¸ ê²€ìƒ‰ê³¼ TF-IDF ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œì— ì‚¬ìš©ë©ë‹ˆë‹¤.

### Elasticsearch ì¸ë±ìŠ¤ êµ¬ì¡°

**ì¸ë±ìŠ¤ ì´ë¦„:** `documents`

**ë§¤í•‘ (Mapping):**
```json
{
  "settings": {
    "analysis": {
      "analyzer": {
        "korean_analyzer": {
          "type": "custom",
          "tokenizer": "standard",
          "filter": ["lowercase"]
        }
      }
    },
    "number_of_shards": 1,
    "number_of_replicas": 0
  },
  "mappings": {
    "properties": {
      "document_id": {"type": "long"},
      "user_id": {"type": "long"},
      "content": {
        "type": "text",
        "analyzer": "korean_analyzer",
        "fielddata": true  # TF-IDF ê³„ì‚°ì„ ìœ„í•´ í•„ìš”
      },
      "filename": {"type": "keyword"},
      "file_type": {"type": "keyword"},
      "uploaded_at": {"type": "date"}
    }
  }
}
```

### Elasticsearch Client êµ¬í˜„ (`core/elasticsearch_client.py`)

**ì£¼ìš” ê¸°ëŠ¥:**

#### 1. ë¬¸ì„œ ìƒ‰ì¸ (Indexing)
```python
async def index_document(document_id, user_id, content, filename, file_type, uploaded_at):
    """
    Elasticsearchì— ë¬¸ì„œ ìƒ‰ì¸

    - ì „ë¬¸ ê²€ìƒ‰ì„ ìœ„í•œ ì—­ìƒ‰ì¸ ìƒì„±
    - TF-IDF ê³„ì‚°ì„ ìœ„í•œ í†µê³„ ìˆ˜ì§‘
    """
    doc_body = {
        "document_id": document_id,
        "user_id": user_id,
        "content": content,              # ì¶”ì¶œëœ í…ìŠ¤íŠ¸
        "filename": filename,
        "file_type": file_type,
        "uploaded_at": uploaded_at
    }

    await client.index(
        index="documents",
        id=str(document_id),
        document=doc_body
    )
```

#### 2. TF-IDF ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
```python
async def extract_significant_terms(document_id, size=3):
    """
    Term Vectors APIë¥¼ ì‚¬ìš©í•œ TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ

    TF-IDF (Term Frequency - Inverse Document Frequency):
    - TF: í•´ë‹¹ ë¬¸ì„œì—ì„œ ë‹¨ì–´ê°€ ë‚˜íƒ€ë‚œ ë¹ˆë„
    - IDF: ì „ì²´ ë¬¸ì„œì—ì„œ ë‹¨ì–´ì˜ í¬ì†Œì„± (í”í•˜ì§€ ì•Šì„ìˆ˜ë¡ ë†’ìŒ)
    - TF-IDF = TF Ã— IDF

    ë†’ì€ TF-IDF ì ìˆ˜ë¥¼ ê°€ì§„ ë‹¨ì–´ = í•´ë‹¹ ë¬¸ì„œë¥¼ ì˜ ëŒ€í‘œí•˜ëŠ” í‚¤ì›Œë“œ
    """
    # 1. Term Vectors API í˜¸ì¶œ
    tv_response = await client.termvectors(
        index="documents",
        id=str(document_id),
        fields=["content"],
        term_statistics=True,      # DF (ì „ì²´ ë¬¸ì„œì—ì„œ ë‹¨ì–´ ë¹ˆë„)
        field_statistics=True      # ì „ì²´ ë¬¸ì„œ ìˆ˜
    )

    # 2. TF-IDF ì ìˆ˜ ê³„ì‚°
    terms = tv_response["term_vectors"]["content"]["terms"]
    term_scores = []

    for term, term_info in terms.items():
        # TF (Term Frequency)
        tf = term_info["term_freq"]

        # DF (Document Frequency)
        df = term_info["doc_freq"]

        # ì „ì²´ ë¬¸ì„œ ìˆ˜
        total_docs = tv_response["term_vectors"]["content"]["field_statistics"]["doc_count"]

        # IDF (Inverse Document Frequency) ê³„ì‚°
        import math
        idf = math.log((total_docs + 1) / (df + 1)) + 1

        # TF-IDF ì ìˆ˜
        tfidf = tf * idf

        # ë‹¨ì–´ ê¸¸ì´ í•„í„° (ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ ë‹¨ì–´ ì œì™¸)
        if 2 <= len(term) <= 30:
            term_scores.append((term, tfidf))

    # 3. ìƒìœ„ Nê°œ í‚¤ì›Œë“œ ë°˜í™˜
    term_scores.sort(key=lambda x: x[1], reverse=True)
    keywords = [term for term, score in term_scores[:size]]

    return keywords
```

#### 3. ì „ë¬¸ ê²€ìƒ‰ (Full-Text Search)
```python
async def search_documents(user_id, query, size=10):
    """
    ì‚¬ìš©ì ë¬¸ì„œ ì „ë¬¸ ê²€ìƒ‰

    - í•œêµ­ì–´ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•œ í† í°í™”
    - BM25 ë­í‚¹ ì•Œê³ ë¦¬ì¦˜ ì ìš©
    """
    search_body = {
        "query": {
            "bool": {
                "must": [
                    {"match": {"content": query}},  # ì „ë¬¸ ê²€ìƒ‰
                    {"term": {"user_id": user_id}}  # ì‚¬ìš©ì í•„í„°
                ]
            }
        },
        "size": size
    }

    response = await client.search(
        index="documents",
        body=search_body
    )

    return response["hits"]["hits"]
```

### Elasticsearch ì¥ì 

1. **í™•ì¥ì„±**: ë¶„ì‚° ì•„í‚¤í…ì²˜ë¡œ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ê°€ëŠ¥
2. **ì‹¤ì‹œê°„ ê²€ìƒ‰**: Near Real-Time ê²€ìƒ‰ ì§€ì›
3. **ë‹¤ì–‘í•œ ë¶„ì„**: TF-IDF, BM25, í˜•íƒœì†Œ ë¶„ì„ ë“± ì§€ì›
4. **RESTful API**: ê°„ë‹¨í•œ HTTP ìš”ì²­ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥
5. **í’ë¶€í•œ ì¿¼ë¦¬ DSL**: ë³µì¡í•œ ê²€ìƒ‰ ì¡°ê±´ í‘œí˜„ ê°€ëŠ¥

### í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

```bash
ELASTICSEARCH_URL=http://localhost:9200
ELASTICSEARCH_USER=elastic
ELASTICSEARCH_PASSWORD=changeme
KEYWORD_EXTRACTION_THRESHOLD=10  # í•˜ì´ë¸Œë¦¬ë“œ ì „í™˜ ì„ê³„ê°’
```

---

## ğŸ¤– KeyBERT (AI í‚¤ì›Œë“œ ì¶”ì¶œ)

KeyBERTëŠ” BERT ì„ë² ë”©ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œì—ì„œ ì˜ë¯¸ ìˆëŠ” í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ëŠ” Python ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.

### KeyBERT ë™ì‘ ì›ë¦¬

**1. BERT ì„ë² ë”© ìƒì„±**
```
ë¬¸ì„œ í…ìŠ¤íŠ¸ â†’ BERT ëª¨ë¸ â†’ ë¬¸ì„œ ì„ë² ë”© (768ì°¨ì› ë²¡í„°)
ê° ë‹¨ì–´/êµ¬ë¬¸ â†’ BERT ëª¨ë¸ â†’ ë‹¨ì–´ ì„ë² ë”© (768ì°¨ì› ë²¡í„°)
```

**2. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°**
```
similarity(ë¬¸ì„œ, í‚¤ì›Œë“œ) = cosine(ë¬¸ì„œ ì„ë² ë”©, í‚¤ì›Œë“œ ì„ë² ë”©)
```

**3. ìƒìœ„ Nê°œ í‚¤ì›Œë“œ ì„ íƒ**
```
ìœ ì‚¬ë„ê°€ ë†’ì€ í‚¤ì›Œë“œ = ë¬¸ì„œì™€ ì˜ë¯¸ì ìœ¼ë¡œ ê´€ë ¨ì„±ì´ ë†’ì€ í‚¤ì›Œë“œ
```

### KeyBERT Extractor êµ¬í˜„ (`core/keyword_extraction.py`)

```python
class KeyBERTExtractor(KeywordExtractor):
    """KeyBERT ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œê¸°"""

    def __init__(self):
        self.model = None

    def _load_model(self):
        """
        KeyBERT ëª¨ë¸ ë¡œë“œ (Lazy Loading)

        - ìµœì´ˆ 1íšŒë§Œ ë¡œë“œ (ë©”ëª¨ë¦¬ ì ˆì•½)
        - sentence-transformers ê¸°ë°˜
        """
        if self.model is None:
            from keybert import KeyBERT
            self.model = KeyBERT()
            logger.info("KeyBERT ëª¨ë¸ ë¡œë“œ ì„±ê³µ")

    async def extract_keywords(self, text, document_id=None):
        """
        KeyBERTë¥¼ ì‚¬ìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œ

        Args:
            text: ëŒ€ìƒ í…ìŠ¤íŠ¸
            document_id: ì‚¬ìš©í•˜ì§€ ì•ŠìŒ

        Returns:
            ì¶”ì¶œëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        """
        self._load_model()

        # KeyBERT í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords_with_scores = self.model.extract_keywords(
            text,
            keyphrase_ngram_range=(1, 2),  # 1~2 ë‹¨ì–´ êµ¬ë¬¸ê¹Œì§€ í‚¤ì›Œë“œë¡œ ê³ ë ¤
            stop_words='english',          # ì˜ì–´ ë¶ˆìš©ì–´ ì œê±° (is, a, the ë“±)
            top_n=3,                        # ìƒìœ„ 3ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ
            use_maxsum=True,                # ë‹¤ì–‘ì„± ì¦ê°€ (í‚¤ì›Œë“œ ì¤‘ë³µ ë°©ì§€)
            nr_candidates=20                # ë‚´ë¶€ í›„ë³´ í‚¤ì›Œë“œ 20ê°œ ìƒì„± í›„ í•„í„°ë§
        )

        # (í‚¤ì›Œë“œ, ì ìˆ˜) íŠœí”Œì—ì„œ í‚¤ì›Œë“œë§Œ ì¶”ì¶œ
        keywords = [kw[0] for kw in keywords_with_scores]

        return keywords
```

### KeyBERT íŒŒë¼ë¯¸í„° ì„¤ëª…

| íŒŒë¼ë¯¸í„° | ì„¤ëª… | ì˜ˆì‹œ |
|---------|------|------|
| `keyphrase_ngram_range` | í‚¤ì›Œë“œ ë‹¨ì–´ ìˆ˜ ë²”ìœ„ | `(1, 2)` â†’ "machine", "machine learning" |
| `stop_words` | ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ | `'english'` â†’ "is", "a", "the" ì œê±° |
| `top_n` | ì¶”ì¶œí•  í‚¤ì›Œë“œ ê°œìˆ˜ | `3` â†’ ìƒìœ„ 3ê°œë§Œ ì¶”ì¶œ |
| `use_maxsum` | ë‹¤ì–‘ì„± ì¦ê°€ | `True` â†’ ì¤‘ë³µëœ ì˜ë¯¸ í‚¤ì›Œë“œ ì œê±° |
| `nr_candidates` | í›„ë³´ í‚¤ì›Œë“œ ìˆ˜ | `20` â†’ 20ê°œ í›„ë³´ ì¤‘ best ì„ íƒ |

### KeyBERT ì¥ì 

1. **ì˜ë¯¸ë¡ ì  ì´í•´**: BERT ì„ë² ë”©ìœ¼ë¡œ ë¬¸ë§¥ ê³ ë ¤
2. **Cold Start ë¬¸ì œ í•´ê²°**: ë‹¨ì¼ ë¬¸ì„œë§Œìœ¼ë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ ê°€ëŠ¥
3. **ë‹¤êµ­ì–´ ì§€ì›**: í•œêµ­ì–´, ì˜ì–´ ë“± ë‹¤ì–‘í•œ ì–¸ì–´ ì§€ì›
4. **ì‚¬ì „ í•™ìŠµ ëª¨ë¸**: ì¶”ê°€ í•™ìŠµ ì—†ì´ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥
5. **ì»¤ìŠ¤í„°ë§ˆì´ì§•**: ë‹¤ì–‘í•œ íŒŒë¼ë¯¸í„°ë¡œ ì¡°ì • ê°€ëŠ¥

### í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

```bash
KEYWORD_EXTRACTION_COUNT=3              # ì¶”ì¶œí•  í‚¤ì›Œë“œ ê°œìˆ˜
KEYWORD_EXTRACTION_THRESHOLD=10         # í•˜ì´ë¸Œë¦¬ë“œ ì „í™˜ ì„ê³„ê°’
```

### KeyBERT vs Elasticsearch ë¹„êµ

| í•­ëª© | KeyBERT | Elasticsearch TF-IDF |
|-----|---------|---------------------|
| **ì‚¬ìš© ì‹œì ** | Cold Start (ë¬¸ì„œ < 10ê°œ) | Normal (ë¬¸ì„œ >= 10ê°œ) |
| **ì›ë¦¬** | BERT ì„ë² ë”© + ì½”ì‚¬ì¸ ìœ ì‚¬ë„ | TF-IDF í†µê³„ |
| **ì¥ì ** | ì˜ë¯¸ë¡ ì  ì •í™•ë„ ë†’ìŒ | ìƒëŒ€ì  ì¤‘ìš”ë„ ë°˜ì˜ |
| **ë‹¨ì ** | ê³„ì‚° ë¹„ìš© ë†’ìŒ | ë¬¸ì„œ ê°„ ë¹„êµ ë°ì´í„° í•„ìš” |
| **ì í•©í•œ ìƒí™©** | ì´ˆê¸° ë°ì´í„° ë¶€ì¡± ì‹œ | ì¶©ë¶„í•œ ë¬¸ì„œ ì¶•ì  í›„ |

---

## ğŸ“– ê°œë°œ ê°€ì´ë“œ

### ìƒˆë¡œìš´ ë„ë©”ì¸ ì¶”ê°€í•˜ê¸°

1. `src/domains/` ì•„ë˜ì— ìƒˆ í´ë” ìƒì„± (ì˜ˆ: `users`)
2. ë‹¤ìŒ íŒŒì¼ë“¤ì„ ìƒì„±:
   - `models.py`: SQLAlchemy ëª¨ë¸ ì •ì˜
   - `schemas.py`: Pydantic ìŠ¤í‚¤ë§ˆ ì •ì˜
   - `repositories.py`: ë°ì´í„° ì•¡ì„¸ìŠ¤ ë¡œì§
   - `services.py`: ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§
   - `router.py`: API ì—”ë“œí¬ì¸íŠ¸
3. `src/main.py`ì— ë¼ìš°í„° ë“±ë¡

```python
from src.domains.users.router import router as users_router
app.include_router(users_router, prefix="/api/users", tags=["Users"])
```

### ì½”ë“œ ìŠ¤íƒ€ì¼

í”„ë¡œì íŠ¸ëŠ” ë‹¤ìŒ ë„êµ¬ë“¤ì„ ì‚¬ìš©í•˜ì—¬ ì½”ë“œ í’ˆì§ˆì„ ìœ ì§€í•©ë‹ˆë‹¤:

```bash
# ì½”ë“œ í¬ë§·íŒ…
black .

# ë¦°íŒ…
flake8 .

# íƒ€ì… ì²´í¬
mypy src/
```

### í…ŒìŠ¤íŠ¸

```bash
# ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
pytest

# íŠ¹ì • í…ŒìŠ¤íŠ¸ íŒŒì¼ ì‹¤í–‰
pytest tests/test_auth.py

# ì»¤ë²„ë¦¬ì§€ í¬í•¨
pytest --cov=src tests/
```

---

## ğŸ“ ë¼ì´ì„¼ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„¼ìŠ¤ í•˜ì— ìˆìŠµë‹ˆë‹¤.

---

## ğŸ‘¥ ê¸°ì—¬

ê¸°ì—¬ë¥¼ í™˜ì˜í•©ë‹ˆë‹¤! Pull Requestë¥¼ ë³´ë‚´ì£¼ì„¸ìš”.

---

## ğŸ“ ë¬¸ì˜

í”„ë¡œì íŠ¸ì— ëŒ€í•œ ë¬¸ì˜ì‚¬í•­ì´ ìˆìœ¼ì‹œë©´ ì´ìŠˆë¥¼ ë“±ë¡í•´ì£¼ì„¸ìš”.
