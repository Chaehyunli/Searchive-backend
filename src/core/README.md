# Core ëª¨ë“ˆ (í•µì‹¬ ì¸í”„ë¼)

`src/core/` í´ë”ëŠ” í”„ë¡œì íŠ¸ ì „ë°˜ì—ì„œ ì‚¬ìš©ë˜ëŠ” í•µì‹¬ ì¸í”„ë¼ ëª¨ë“ˆë“¤ì„ í¬í•¨í•©ë‹ˆë‹¤.

---

## ğŸ“‚ ëª¨ë“ˆ êµ¬ì¡°

```
src/core/
â”œâ”€â”€ config.py                    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
â”œâ”€â”€ security.py                  # JWT ë³´ì•ˆ ìœ í‹¸ë¦¬í‹°
â”œâ”€â”€ exception.py                 # ì „ì—­ ì˜ˆì™¸ ì²˜ë¦¬
â”œâ”€â”€ redis.py                     # Redis ì„¸ì…˜ ê´€ë¦¬
â”œâ”€â”€ minio_client.py              # MinIO ê°ì²´ ìŠ¤í† ë¦¬ì§€ í´ë¼ì´ì–¸íŠ¸
â”œâ”€â”€ elasticsearch_client.py      # Elasticsearch ê²€ìƒ‰ ì—”ì§„ í´ë¼ì´ì–¸íŠ¸
â”œâ”€â”€ text_extractor.py            # íŒŒì¼ â†’ í…ìŠ¤íŠ¸ ì¶”ì¶œê¸°
â””â”€â”€ keyword_extraction.py        # AI í‚¤ì›Œë“œ ì¶”ì¶œ ì„œë¹„ìŠ¤
```

---

## ğŸ—„ï¸ MinIO (ê°ì²´ ìŠ¤í† ë¦¬ì§€)

### ê°œìš”

MinIOëŠ” Amazon S3 í˜¸í™˜ APIë¥¼ ì œê³µí•˜ëŠ” ì˜¤í”ˆì†ŒìŠ¤ ê°ì²´ ìŠ¤í† ë¦¬ì§€ì…ë‹ˆë‹¤. Searchiveì—ì„œëŠ” ì‚¬ìš©ì ì—…ë¡œë“œ ë¬¸ì„œë¥¼ ì €ì¥í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.

### ë²„í‚· êµ¬ì¡°

```
user-documents/              # ë²„í‚· ì´ë¦„
  â”œâ”€â”€ 1/                     # user_id=1
  â”‚   â”œâ”€â”€ a1b2c3d4-uuid.pdf
  â”‚   â”œâ”€â”€ e5f6g7h8-uuid.docx
  â”‚   â””â”€â”€ i9j0k1l2-uuid.xlsx
  â”œâ”€â”€ 2/                     # user_id=2
  â”‚   â”œâ”€â”€ m3n4o5p6-uuid.pdf
  â”‚   â””â”€â”€ q7r8s9t0-uuid.txt
  â””â”€â”€ 3/                     # user_id=3
      â””â”€â”€ u1v2w3x4-uuid.pptx
```

- ê° ì‚¬ìš©ìëŠ” `user_id` í´ë”ë¡œ ê²©ë¦¬ë©ë‹ˆë‹¤.
- íŒŒì¼ëª…ì€ **UUID(ê³ ìœ  ëœë¤ê°’)**ë¡œ ì €ì¥ë˜ì–´ ì¶©ëŒê³¼ ì¶”ì¸¡ì„ ë°©ì§€í•©ë‹ˆë‹¤.
- ì›ë³¸ íŒŒì¼ëª…ì€ PostgreSQLì˜ `original_filename` ì»¬ëŸ¼ì— ì €ì¥ë©ë‹ˆë‹¤.

### MinIO Client êµ¬í˜„ (`minio_client.py`)

#### 1. íŒŒì¼ ì—…ë¡œë“œ

```python
def upload_file(file_path: str, file_data: BytesIO, file_size: int, content_type: str):
    """
    MinIOì— íŒŒì¼ ì—…ë¡œë“œ

    Args:
        file_path: ì €ì¥ ê²½ë¡œ (ì˜ˆ: "1/uuid.pdf")
        file_data: íŒŒì¼ ë°”ì´íŠ¸ ìŠ¤íŠ¸ë¦¼
        file_size: íŒŒì¼ í¬ê¸° (bytes)
        content_type: MIME íƒ€ì… (ì˜ˆ: "application/pdf")
    """
    client.put_object(
        bucket_name=BUCKET_NAME,
        object_name=file_path,
        data=file_data,
        length=file_size,
        content_type=content_type
    )
```

#### 2. íŒŒì¼ ë‹¤ìš´ë¡œë“œ

```python
def download_file(file_path: str) -> bytes:
    """
    MinIOì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ

    Returns:
        íŒŒì¼ ë°”ì´íŠ¸ ë°ì´í„°
    """
    response = client.get_object(
        bucket_name=BUCKET_NAME,
        object_name=file_path
    )
    return response.read()
```

#### 3. íŒŒì¼ ì‚­ì œ

```python
def delete_file(file_path: str):
    """
    MinIOì—ì„œ íŒŒì¼ ì‚­ì œ
    """
    client.remove_object(
        bucket_name=BUCKET_NAME,
        object_name=file_path
    )
```

### í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

```bash
MINIO_ENDPOINT=localhost:9000
MINIO_ACCESS_KEY=minioadmin
MINIO_SECRET_KEY=minioadmin
MINIO_BUCKET_NAME=user-documents
MINIO_SECURE=False  # HTTPS ì‚¬ìš© ì—¬ë¶€
```

---

## ğŸ” Elasticsearch (ê²€ìƒ‰ ì—”ì§„)

### ê°œìš”

ElasticsearchëŠ” ë¶„ì‚°í˜• ê²€ìƒ‰ ë° ë¶„ì„ ì—”ì§„ì…ë‹ˆë‹¤. Searchiveì—ì„œëŠ” ë¬¸ì„œ ì „ë¬¸ ê²€ìƒ‰ê³¼ TF-IDF ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œì— ì‚¬ìš©ë©ë‹ˆë‹¤.

### ì¸ë±ìŠ¤ êµ¬ì¡°

**ì¸ë±ìŠ¤ ì´ë¦„:** `documents`

**ë§¤í•‘ (Mapping):**
```json
{
  "settings": {
    "analysis": {
      "analyzer": {
        "korean_nori_analyzer": {
          "type": "custom",
          "tokenizer": "nori_tokenizer",
          "filter": ["nori_pos_filter", "lowercase"]
        }
      },
      "filter": {
        "nori_pos_filter": {
          "type": "nori_part_of_speech",
          "stoptags": ["J", "E", "IC", "MAJ", "MM", "SP", "SSC", "SSO", ...]
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "document_id": {"type": "long"},
      "user_id": {"type": "long"},
      "content": {
        "type": "text",
        "analyzer": "korean_nori_analyzer",
        "fielddata": true
      },
      "filename": {"type": "keyword"},
      "file_type": {"type": "keyword"},
      "uploaded_at": {"type": "date"}
    }
  }
}
```

### Nori í˜•íƒœì†Œ ë¶„ì„ê¸° ì„¤ì •

Elasticsearchì—ì„œ í•œêµ­ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ í’ˆì§ˆì„ ê°œì„ í•˜ê¸° ìœ„í•´ **Nori í˜•íƒœì†Œ ë¶„ì„ê¸°**ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

**ë¬¸ì œì :**
- ê¸°ë³¸ ë¶„ì„ê¸° ì‚¬ìš© ì‹œ "ì„", "ë¥¼", "ê³¼", "ì™€", "ê²ƒì´" ê°™ì€ ì¡°ì‚¬ì™€ ì–´ë¯¸ê°€ í‚¤ì›Œë“œë¡œ ì¶”ì¶œë¨

**í•´ê²°ì±…:**
- Nori í”ŒëŸ¬ê·¸ì¸ì„ ì„¤ì¹˜í•˜ê³  39ê°œì˜ í’ˆì‚¬ íƒœê·¸ë¥¼ í•„í„°ë§í•˜ì—¬ ë¶ˆí•„ìš”í•œ ì¡°ì‚¬, ì–´ë¯¸, ì ‘ì‚¬, ê¸°í˜¸ ë“±ì„ ì œê±°

**ìì„¸í•œ ì„¤ì • ê°€ì´ë“œ:** [`docs/NORI_SETUP.md`](../../docs/NORI_SETUP.md) ì°¸ê³ 

### Elasticsearch Client êµ¬í˜„ (`elasticsearch_client.py`)

#### 1. ë¬¸ì„œ ìƒ‰ì¸ (Indexing)

```python
async def index_document(document_id, user_id, content, filename, file_type, uploaded_at):
    """
    Elasticsearchì— ë¬¸ì„œ ìƒ‰ì¸

    - ì „ë¬¸ ê²€ìƒ‰ì„ ìœ„í•œ ì—­ìƒ‰ì¸ ìƒì„±
    - TF-IDF ê³„ì‚°ì„ ìœ„í•œ í†µê³„ ìˆ˜ì§‘
    """
    doc_body = {
        "document_id": document_id,
        "user_id": user_id,
        "content": content,
        "filename": filename,
        "file_type": file_type,
        "uploaded_at": uploaded_at
    }

    await client.index(
        index="documents",
        id=str(document_id),
        document=doc_body
    )
```

#### 2. TF-IDF ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ

```python
async def extract_significant_terms(document_id, size=3):
    """
    Term Vectors APIë¥¼ ì‚¬ìš©í•œ TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ

    TF-IDF (Term Frequency - Inverse Document Frequency):
    - TF: í•´ë‹¹ ë¬¸ì„œì—ì„œ ë‹¨ì–´ê°€ ë‚˜íƒ€ë‚œ ë¹ˆë„
    - IDF: ì „ì²´ ë¬¸ì„œì—ì„œ ë‹¨ì–´ì˜ í¬ì†Œì„± (í”í•˜ì§€ ì•Šì„ìˆ˜ë¡ ë†’ìŒ)
    - TF-IDF = TF Ã— IDF

    ë†’ì€ TF-IDF ì ìˆ˜ë¥¼ ê°€ì§„ ë‹¨ì–´ = í•´ë‹¹ ë¬¸ì„œë¥¼ ì˜ ëŒ€í‘œí•˜ëŠ” í‚¤ì›Œë“œ
    """
    # Term Vectors API í˜¸ì¶œ
    tv_response = await client.termvectors(
        index="documents",
        id=str(document_id),
        fields=["content"],
        term_statistics=True,
        field_statistics=True
    )

    # TF-IDF ì ìˆ˜ ê³„ì‚° ë° ìƒìœ„ Nê°œ ë°˜í™˜
    # ... (ìƒì„¸ êµ¬í˜„ì€ elasticsearch_client.py:269-340 ì°¸ê³ )
```

#### 3. ì „ë¬¸ ê²€ìƒ‰ (Full-Text Search)

```python
async def search_documents(user_id, query, size=10):
    """
    ì‚¬ìš©ì ë¬¸ì„œ ì „ë¬¸ ê²€ìƒ‰

    - í•œêµ­ì–´ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•œ í† í°í™”
    - BM25 ë­í‚¹ ì•Œê³ ë¦¬ì¦˜ ì ìš©
    """
    search_body = {
        "query": {
            "bool": {
                "must": [
                    {"match": {"content": query}},
                    {"term": {"user_id": user_id}}
                ]
            }
        },
        "size": size
    }

    response = await client.search(
        index="documents",
        body=search_body
    )

    return response["hits"]["hits"]
```

### í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

```bash
ELASTICSEARCH_URL=http://localhost:9200
ELASTICSEARCH_USER=elastic
ELASTICSEARCH_PASSWORD=changeme
KEYWORD_EXTRACTION_THRESHOLD=10  # í•˜ì´ë¸Œë¦¬ë“œ ì „í™˜ ì„ê³„ê°’
```

---

## ğŸ“„ TextExtractor (í…ìŠ¤íŠ¸ ì¶”ì¶œê¸°)

### ê°œìš”

`text_extractor.py`ëŠ” ë‹¤ì–‘í•œ íŒŒì¼ í˜•ì‹(PDF, DOCX, XLSX, PPTX, TXT, HWP ë“±)ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” ëª¨ë“ˆì…ë‹ˆë‹¤.

### ì§€ì› íŒŒì¼ í˜•ì‹

| í˜•ì‹ | MIME Type | ë¼ì´ë¸ŒëŸ¬ë¦¬ |
|------|-----------|-----------|
| PDF | `application/pdf` | `pypdf` |
| DOCX | `application/vnd.openxmlformats-officedocument.wordprocessingml.document` | `python-docx` |
| XLSX | `application/vnd.openxmlformats-officedocument.spreadsheetml.sheet` | `openpyxl` |
| PPTX | `application/vnd.openxmlformats-officedocument.presentationml.presentation` | `python-pptx` |
| TXT | `text/plain` | UTF-8/CP949 ë””ì½”ë”© |
| HWP | `application/x-hwp`, `application/haansofthwp` | `olefile` |

### êµ¬í˜„ (`text_extractor.py`)

#### ë©”ì¸ ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸

```python
@staticmethod
def extract_text_from_bytes(file_data: bytes, file_type: str, filename: str) -> Optional[str]:
    """
    íŒŒì¼ íƒ€ì…ì— ë”°ë¼ ì ì ˆí•œ ì¶”ì¶œê¸° í˜¸ì¶œ
    """
    if file_type == "application/pdf":
        return TextExtractor._extract_from_pdf(file_data)
    elif file_type == "text/plain":
        return TextExtractor._extract_from_txt(file_data)
    elif "wordprocessing" in file_type:
        return TextExtractor._extract_from_docx(file_data)
    elif "spreadsheet" in file_type:
        return TextExtractor._extract_from_excel(file_data)
    elif "presentation" in file_type:
        return TextExtractor._extract_from_pptx(file_data)
    elif "hwp" in file_type or "haansoft" in file_type:
        return TextExtractor._extract_from_hwp(file_data)
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_type}")
```

#### HWP íŒŒì¼ ì²˜ë¦¬

HWP íŒŒì¼ì€ OLE êµ¬ì¡°ë¥¼ ê°€ì§€ë©°, ë‚´ë¶€ì ìœ¼ë¡œ zlib ì••ì¶•ëœ BodyText ì„¹ì…˜ì„ í¬í•¨í•©ë‹ˆë‹¤.

```python
@staticmethod
def _extract_from_hwp(file_data: bytes) -> Optional[str]:
    """
    HWP íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ

    1. OLE íŒŒì¼ ì—´ê¸°
    2. BodyText/* ì„¹ì…˜ ì°¾ê¸° ë° ì •ë ¬
    3. zlib ì••ì¶• í•´ì œ
    4. ë ˆì½”ë“œ íŒŒì‹± (Record ID 67 = HWPTAG_PARA_TEXT)
    5. UTF-16LE ë””ì½”ë”©
    """
    import olefile, zlib, struct

    ole = olefile.OleFileIO(BytesIO(file_data))

    # BodyText ì„¹ì…˜ ì¶”ì¶œ ë° ì •ë ¬
    bodytext_streams = sorted(
        [s for s in ole.listdir() if s[0] == "BodyText"],
        key=lambda x: int(x[1].replace("Section", ""))
    )

    text_parts = []
    for stream in bodytext_streams:
        compressed_data = ole.openstream(stream).read()
        unpacked = zlib.decompress(compressed_data, -15)
        text_parts.append(TextExtractor._parse_hwp_text(unpacked))

    return "\n".join(text_parts)

@staticmethod
def _parse_hwp_text(data: bytes) -> str:
    """
    HWP ë°”ì´ë„ˆë¦¬ì—ì„œ í…ìŠ¤íŠ¸ ë ˆì½”ë“œ íŒŒì‹±

    ë ˆì½”ë“œ êµ¬ì¡°:
    - 4 bytes: í—¤ë” (í•˜ìœ„ 10ë¹„íŠ¸ = Record ID)
    - 4 bytes: ë ˆì½”ë“œ í¬ê¸°
    - N bytes: ë ˆì½”ë“œ ë°ì´í„°
    """
    text_parts = []
    pos = 0

    while pos < len(data):
        record_header = struct.unpack("<I", data[pos:pos+4])[0]
        record_id = record_header & 0x3FF
        record_size = struct.unpack("<I", data[pos+4:pos+8])[0]

        if record_id == 67:  # HWPTAG_PARA_TEXT
            record_data = data[pos+8:pos+8+record_size]
            text = record_data.decode('utf-16le', errors='ignore')
            text_parts.append(text.replace('\x00', ''))

        pos += 8 + record_size

    return "".join(text_parts)
```

---

## ğŸ¤– KeyBERT (AI í‚¤ì›Œë“œ ì¶”ì¶œ)

### ê°œìš”

KeyBERTëŠ” BERT ì„ë² ë”©ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œì—ì„œ ì˜ë¯¸ ìˆëŠ” í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ëŠ” Python ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.

### ë™ì‘ ì›ë¦¬

**1. BERT ì„ë² ë”© ìƒì„±**
```
ë¬¸ì„œ í…ìŠ¤íŠ¸ â†’ BERT ëª¨ë¸ â†’ ë¬¸ì„œ ì„ë² ë”© (768ì°¨ì› ë²¡í„°)
ê° ë‹¨ì–´/êµ¬ë¬¸ â†’ BERT ëª¨ë¸ â†’ ë‹¨ì–´ ì„ë² ë”© (768ì°¨ì› ë²¡í„°)
```

**2. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°**
```
similarity(ë¬¸ì„œ, í‚¤ì›Œë“œ) = cosine(ë¬¸ì„œ ì„ë² ë”©, í‚¤ì›Œë“œ ì„ë² ë”©)
```

**3. ìƒìœ„ Nê°œ í‚¤ì›Œë“œ ì„ íƒ**
```
ìœ ì‚¬ë„ê°€ ë†’ì€ í‚¤ì›Œë“œ = ë¬¸ì„œì™€ ì˜ë¯¸ì ìœ¼ë¡œ ê´€ë ¨ì„±ì´ ë†’ì€ í‚¤ì›Œë“œ
```

### í•˜ì´ë¸Œë¦¬ë“œ í‚¤ì›Œë“œ ì¶”ì¶œ ì „ëµ (`keyword_extraction.py`)

SearchiveëŠ” **Cold Start â†’ Normal** ì „í™˜ ì „ëµì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

#### 1. Cold Start ëª¨ë“œ (ë¬¸ì„œ ìˆ˜ < 10ê°œ)

- **ì¶”ì¶œ ë°©ë²•**: KeyBERT (BERT ê¸°ë°˜ ì„ë² ë”©)
- **ì¥ì **: ë¬¸ì„œ ê°„ ë¹„êµ ë°ì´í„° ë¶€ì¡± ì‹œì—ë„ ë‹¨ì¼ ë¬¸ì„œì—ì„œ ì˜ë¯¸ ìˆëŠ” í‚¤ì›Œë“œ ì¶”ì¶œ
- **ì¶”ì¶œ ê³¼ì •**:

```python
class KeyBERTExtractor(KeywordExtractor):
    async def extract_keywords(self, text, document_id=None):
        self._load_model()  # Lazy Loading

        keywords_with_scores = self.model.extract_keywords(
            text,
            keyphrase_ngram_range=(1, 2),  # 1~2 ë‹¨ì–´ êµ¬ë¬¸ê¹Œì§€
            stop_words='english',          # ë¶ˆìš©ì–´ ì œê±°
            top_n=3,                        # ìƒìœ„ 3ê°œ
            use_maxsum=True,                # ë‹¤ì–‘ì„± ì¦ê°€
            nr_candidates=20                # í›„ë³´ 20ê°œ ìƒì„±
        )

        return [kw[0] for kw in keywords_with_scores]
```

#### 2. Normal ëª¨ë“œ (ë¬¸ì„œ ìˆ˜ >= 10ê°œ)

- **ì¶”ì¶œ ë°©ë²•**: Elasticsearch TF-IDF
- **ì¥ì **: ì „ì²´ ë¬¸ì„œ ì»¬ë ‰ì…˜ê³¼ ë¹„êµí•˜ì—¬ ìƒëŒ€ì  ì¤‘ìš”ë„ ê³„ì‚°

```python
class ElasticsearchExtractor(KeywordExtractor):
    async def extract_keywords(self, text, document_id):
        keywords = await elasticsearch_client.extract_significant_terms(
            document_id=document_id,
            size=3
        )
        return keywords
```

#### 3. Hybrid Orchestrator

```python
class HybridKeywordExtractionService:
    async def extract_keywords(self, text, document_id):
        # 1. í˜„ì¬ Elasticsearch ë¬¸ì„œ ìˆ˜ í™•ì¸
        document_count = await elasticsearch_client.get_document_count()

        # 2. ì„ê³„ê°’ ê¸°ë°˜ ì „ëµ ì„ íƒ
        if document_count < self.threshold:
            keywords = await keybert_extractor.extract_keywords(text)
            method = "keybert"
        else:
            keywords = await elasticsearch_extractor.extract_keywords(text, document_id)
            method = "elasticsearch"

        # 3. ì •ê·œí™” (ì†Œë¬¸ì, ì¤‘ë³µ ì œê±°)
        keywords = list(set(kw.strip().lower() for kw in keywords))

        return keywords, method
```

### KeyBERT vs Elasticsearch ë¹„êµ

| í•­ëª© | KeyBERT | Elasticsearch TF-IDF |
|-----|---------|---------------------|
| **ì‚¬ìš© ì‹œì ** | Cold Start (ë¬¸ì„œ < 10ê°œ) | Normal (ë¬¸ì„œ >= 10ê°œ) |
| **ì›ë¦¬** | BERT ì„ë² ë”© + ì½”ì‚¬ì¸ ìœ ì‚¬ë„ | TF-IDF í†µê³„ |
| **ì¥ì ** | ì˜ë¯¸ë¡ ì  ì •í™•ë„ ë†’ìŒ | ìƒëŒ€ì  ì¤‘ìš”ë„ ë°˜ì˜ |
| **ë‹¨ì ** | ê³„ì‚° ë¹„ìš© ë†’ìŒ | ë¬¸ì„œ ê°„ ë¹„êµ ë°ì´í„° í•„ìš” |

### í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

```bash
KEYWORD_EXTRACTION_COUNT=3              # ì¶”ì¶œí•  í‚¤ì›Œë“œ ê°œìˆ˜
KEYWORD_EXTRACTION_THRESHOLD=10         # í•˜ì´ë¸Œë¦¬ë“œ ì „í™˜ ì„ê³„ê°’
```

---

## ğŸ“š ì°¸ê³  ìë£Œ

- [MinIO ê³µì‹ ë¬¸ì„œ](https://min.io/docs/minio/kubernetes/upstream/)
- [Elasticsearch Python Client](https://www.elastic.co/guide/en/elasticsearch/client/python-api/current/index.html)
- [KeyBERT ê³µì‹ ë¬¸ì„œ](https://maartengr.github.io/KeyBERT/)
- [Nori í˜•íƒœì†Œ ë¶„ì„ê¸° ì„¤ì • ê°€ì´ë“œ](../../docs/NORI_SETUP.md)
